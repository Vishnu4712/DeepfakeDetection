{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# !pip install av numpy transformers torch"]},{"cell_type":"markdown","metadata":{},"source":["# Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import av\n","import numpy as np\n","from transformers import VivitImageProcessor, VivitModel\n","import torch\n","import os\n","import numpy as np\n","import gc"]},{"cell_type":"markdown","metadata":{},"source":["# Loading cropped videos"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_path=\"/kaggle/input/vid-deepfake/cropped_celeb_df_fake\"\n","celeb_df_fake_vids=[os.path.join(base_path,vid) for vid in os.listdir(base_path) if vid.endswith(\".mp4\")]\n","celeb_df_fake_vids"]},{"cell_type":"markdown","metadata":{},"source":["# Function to read cropped videos and sample frames"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.random.seed(0)\n","\n","\n","def read_video_pyav(container, indices):\n","    frames = []\n","    container.seek(0)\n","    start_index = indices[0]\n","    end_index = indices[-1]\n","    for i, frame in enumerate(container.decode(video=0)):\n","        if i > end_index:\n","            break\n","        if i >= start_index and i in indices:\n","            frames.append(frame)\n","    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n","\n","def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n","\n","    converted_len = int(clip_len * frame_sample_rate)\n","    if converted_len >= seg_len:\n","        raise ValueError(\"Not enough frames in the video to sample the specified number of frames.\")\n","    end_idx = np.random.randint(converted_len, seg_len)\n","    start_idx = end_idx - converted_len\n","    indices = np.linspace(start_idx, end_idx, num=clip_len)\n","    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n","    return indices\n"]},{"cell_type":"markdown","metadata":{},"source":["# Function to extract the kinetics-400 pretrained ViViT model embeddings from videos "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T20:34:59.874299Z","iopub.status.busy":"2024-06-18T20:34:59.873914Z","iopub.status.idle":"2024-06-18T20:34:59.881661Z","shell.execute_reply":"2024-06-18T20:34:59.880533Z","shell.execute_reply.started":"2024-06-18T20:34:59.874271Z"},"trusted":true},"outputs":[],"source":["def extract_video_embeddings(video_path, model, processor, device, clip_len=32, frame_sample_rate=1):\n","    container = av.open(video_path)\n","    try:\n","        indices = sample_frame_indices(clip_len, frame_sample_rate, container.streams.video[0].frames)\n","    except ValueError as e:\n","        print(f\"Skipping {video_path} due to error: {e}\")\n","        return None\n","    video = read_video_pyav(container=container, indices=indices)\n","\n","    inputs = processor(list(video), return_tensors=\"pt\").to(device)\n","    with torch.no_grad(): \n","        outputs = model(**inputs)\n","    last_hidden_states = outputs.last_hidden_state\n","    return last_hidden_states"]},{"cell_type":"markdown","metadata":{},"source":["# Making output directory"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T20:35:03.055489Z","iopub.status.busy":"2024-06-18T20:35:03.054734Z","iopub.status.idle":"2024-06-18T20:35:03.060597Z","shell.execute_reply":"2024-06-18T20:35:03.059515Z","shell.execute_reply.started":"2024-06-18T20:35:03.055445Z"},"trusted":true},"outputs":[],"source":["output_dir=\"/kaggle/working/final_celeb_df_fake_video_embeddings\"\n","os.makedirs(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Extacting embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T20:35:27.643384Z","iopub.status.busy":"2024-06-18T20:35:27.642535Z","iopub.status.idle":"2024-06-18T21:11:51.669144Z","shell.execute_reply":"2024-06-18T21:11:51.668168Z","shell.execute_reply.started":"2024-06-18T20:35:27.643348Z"},"trusted":true},"outputs":[],"source":["def store_embeddings(embeddings, filename):\n","    torch.save(embeddings, filename)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","# # Iterate through video paths and extract embeddings\n","image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n","model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\").to(device)\n","\n","total=len(celeb_df_fake_vids)\n","\n","c=1\n","for video_path in celeb_df_fake_vids:\n","    try:\n","        embeddings = extract_video_embeddings(video_path, model, image_processor, device)\n","        if embeddings is not None:\n","            store_embeddings(embeddings, os.path.join(output_dir, f\"embeddings_{os.path.basename(video_path)[:-8]}.pt\"))  \n","            print(\"done with \", c, \" out of \", total)\n","        else:\n","            print(\"embeddings none for....\",video_path)\n","    except RuntimeError as e:\n","        print(f\"Error processing {video_path}: {e}\")\n","    finally:\n","        # Free up GPU memory\n","        del embeddings\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    c += 1\n","print(\"Finished processing all videos!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T21:11:51.671919Z","iopub.status.busy":"2024-06-18T21:11:51.671078Z","iopub.status.idle":"2024-06-18T21:21:22.712372Z","shell.execute_reply":"2024-06-18T21:21:22.711418Z","shell.execute_reply.started":"2024-06-18T21:11:51.671882Z"},"trusted":true},"outputs":[],"source":["!zip -r final_celeb_df_fake_video_embeddings.zip /kaggle/working/final_celeb_df_fake_video_embeddings"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5234817,"sourceId":8723272,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
